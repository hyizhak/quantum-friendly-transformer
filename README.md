# quantum-friendly-transformer

This repo serves as numerical experiments of quantum-friendly-transformer where we spectral-normalize / frobenius-normalize layers of transformers to understand how the block encoding affects model performance.

## Matrix Norms in Transformers

Before experiments showcasing the capability of normalized transformers, we provide scripts to check the norms of pretrained models, check `alpha_*` files:

1. To check the norms of inputs S, in `alpha_s`:

    To see the plots, run `plot_data.ipynb`

    The data is generated by running:

    ``` 
    python alpha_s.py {model_name} {max_tokens} {batch_size} {random}
    ```

    where:

    model_name: ['bert', 'bart', 'roberta', 'distilgpt', 'gpt2', 'gpt', 'llama2-7b', 'tinyllama', 'mistral7b']
    max_tokens: integer for maximum number of tokens, set to 512. 

    batch_size: varies among models, larger models will need a smaller batch size to deal with OOM issues. If batch_size == -1, it means there is no batch_size and we do not truncate the tokens.

    random: 1 or 0 as a boolean variable to run the random dataset (1) or the MMLU dataset (0)

2. To check the norms of parameters W, in `alpha_w`:
    The statistics of the norms of parameters W are generated by running:

    ```
    python wqkv_norm_per_layer.py
    ```

    for a specific layer, you can check the norms of parameters W in that layer by running:

    ```
    python wqkv_norm.py
    ```

## Training and Fine-tuning Normalized Transformers

After setting up the environment with (you may need to ensure your conda installation):

```
bash setup.sh
```

And make sure you have changed output directory in files under `src/quantum_friendly_transformer/trainer` to your desired location,

To train an single-layer transformer on the [Genomic Benchmarks non-tata promoter dataset](https://bmcgenomdata.biomedcentral.com/articles/10.1186/s12863-023-01123-8)(0-1 classification task), use:

```
python src/quantum_friendly_transformer/trainer/train_genomic_bench.py
```

This result is reported in our paper and key hyperparameters are:
- `--learning-rate`: `8e-4`
- `--num-epochs`: `100`
- `--early-stopping`: `10`
- `--weight-decay`: `0.01`
- `--lr-scheduler-type`: `cosine`

We noted that more intricate hyperparameter tuning may be needed for the normalized models, as the optimization is slightly off the sweet point of current optimizers. For more details, please refer to the codebase.

To fine-tune the transformer on the same dataset with specific layer normalized, use:

```
python src/quantum_friendly_transformer/trainer/fine_tune_sn_model_genomic_bench.py
```

To fine-tune the frobenius normalized DNABert on the same dataset, use:

```
python src/quantum_friendly_transformer/trainer/fine_tune_multilayer_genomic_bench.py
```

This result is reported in our paper and key hyperparameters are:
- `--learning-rate`: `2e-4`
- `--num-epochs`: `200`
- `--early-stopping`: `25`
- `--weight-decay`: `0.01`
- `--lr-scheduler-type`: `cosine`
  
For more details, please refer to the codebase.

To train an one-layer transformer on the [Genome Understanding Evaluation (GUE) notata promoter dataset](https://huggingface.co/datasets/leannmlindsey/GUE)(0-1 classification task), use:

```
python src/quantum_friendly_transformer/trainer/train_gue.py
```

To fine-tune the transformer on the same dataset with specific layer normalized, use:

```
python src/quantum_friendly_transformer/trainer/fine_tune_sn_model_gue.py
```

To fine-tune the frobenius normalized DNABert on the same dataset, use:

```
python src/quantum_friendly_transformer/trainer/fine_tune_multilayer_gue.py
```

To train an one-layer transformer on the [conll 2003 dataset](https://huggingface.co/datasets/eriktks/conll2003)(POS classification task), use:

```
python src/quantum_friendly_transformer/trainer/train_conll2003.py
```

To fine-tune the transformer on the same dataset with specific layer normalized, use:

```
python src/quantum_friendly_transformer/trainer/fine_tune_sn_model_conll2003.py
```

